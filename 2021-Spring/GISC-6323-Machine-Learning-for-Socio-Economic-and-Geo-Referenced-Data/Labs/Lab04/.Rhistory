# Print results
credit_svm_auc$results
confusionMatrix(credit_svm_auc)
library(AmesHousing); library(rsample);library(recipes)
ames <- make_ames()
# Stratified Random Sampling
amesSplit <- initial_split(ames, breaks=8, prop=0.8, strata="Sale_Price")
ames_train <- training(amesSplit)
ames_test <- testing(amesSplit)
# Data preprocessing
# blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
#   step_nzv(all_nominal())  %>%
#   step_integer(matches("Qual|Cond|QC|Qu")) %>%
#   step_center(all_numeric(), -all_outcomes()) %>%
#   step_scale(all_numeric(), -all_outcomes()) %>%
#   step_pca(all_numeric(), -all_outcomes())
# prepare <- prep(blueprint, training = ames_train)
# baked_train <- bake(prepare, new_data = ames_train)
# baked_test <- bake(prepare, new_data = ames_test)
# # baked_train
set.seed(1)
ames_svm <- train(
Sale_Price ~ .,
data = ames_train,
method = "svmRadial",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 5),
tuneLength = 10
)
ggplot(ames_svm) + theme_light()
ames_svm$bestTune
y.pred <- predict(ames_svm,newdata = ames_test)
car::scatterplotMatrix(y.pred,ames_test$Sale_Price)
y.pred <- predict(ames_svm,newdata = ames_test)
car::scatterplotMatrix(y.pred,ames_test$Sale_Price)
length(ames_test$Sale_Price)
car::scatterplotMatrix(y.pred,ames_test$Sale_Price)
ggplot(y.pred,ames_test$Sale_Price) + geom_point()
plot(y.pred,ames_test$Sale_Price)
scatterplot(y.pred,ames_test$Sale_Price)
ggplot(aes(x=y.pred, y=ames_test$Sale_Price)) +
geom_point()
svm_result <- data.frame(predict = y.pred, observed = ames_test$Sale_Price)
ggplot(svm_result,aes(x=predict, y=observed)) +
geom_point()
ggplot(svm_result,aes(x=predict, y=observed)) +
geom_point() +
smooth()
y.pred <- predict(ames_svm,newdata = ames_test)
svm_result <- data.frame(predict = y.pred, observed = ames_test$Sale_Price)
ggplot(svm_result,aes(x=predict, y=observed)) +
geom_point() +
geom_smooth(method=lm, se=FALSE, fullrange=TRUE)
y.pred <- predict(ames_svm,newdata = ames_test)
svm_result <- data.frame(predict = y.pred, observed = ames_test$Sale_Price)
ggplot(svm_result,aes(x=predict, y=observed)) +
geom_point() +
geom_smooth(method=lm, se=FALSE, fullrange=TRUE)
plot(x1,x2,col = as.factor(y))
train_pred <- fitted(logistic.mod1)
train_pred <- logistic_as_factor(train_pred)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_pred, color = train_pred), size = 3, alpha = 0.75)
logistic_as_factor <- function(result){
return(as.factor(ifelse(result > 0.5,1,0)))
}
logistic.pred <- predict(logistic.mod1,newdata = df_test,type = "response")
logistic.pred <- logistic_as_factor(logistic.pred)
confusionMatrix(logistic.pred,df_test$y)
set.seed(1)
df <- data.frame(x1 = x1,x2 = x2, y = as.factor(y))
index <- caret::createDataPartition(df$y, p = 0.7, list = FALSE)
df_train <- df[index, ]
df_test  <- df[-index, ]
logistic.mod1 <- glm(y ~ ., family = binomial(link="logit"), data = df_train)
summary(logistic.mod1)
logistic_as_factor <- function(result){
return(as.factor(ifelse(result > 0.5,1,0)))
}
logistic.pred <- predict(logistic.mod1,newdata = df_test,type = "response")
logistic.pred <- logistic_as_factor(logistic.pred)
confusionMatrix(logistic.pred,df_test$y)
train_pred <- fitted(logistic.mod1)
train_pred <- logistic_as_factor(train_pred)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_pred, color = train_pred), size = 3, alpha = 0.75)
x1_quad <- df_train$x1^2
x1.x2 <- df_train$x1 * df_train$x2
logistic.mod2 <- glm(y ~ . + x1_quad + x1.x2 , family = binomial(link="logit"), data = df_train)
summary(logistic.mod2)
df_test$x1_quad <- df_test$x1^2
df_test$x1.x2 <- df_test$x1 * df_test$x2
logistic.pred <- predict(logistic.mod2,newdata = df_test,type = "response")
logistic.pred <- logistic_as_factor(logistic.pred)
confusionMatrix(logistic.pred,df_test$y)
train_pred <- fitted(logistic.mod2)
train_pred <- logistic_as_factor(train_pred)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_pred, color = train_pred), size = 3, alpha = 0.75)
set.seed(1)
tune.out <- tune(svm, y~.,data=df_train,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
ypred <- predict(bestmod, df_test)
confusionMatrix(ypred, df_test$y)
train_y <- fitted(bestmod)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_y, color = train_y), size = 3, alpha = 0.75)
GLM.01 <- glm(lfp ~ k5 + k618 + age + wc + hc + lwg + inc, family=binomial(logit), trace=TRUE, data=Mroz)
# load helper package
library(car)
data(Mroz)
attach(Mroz)
# load helper package
library(car)
data(Mroz)
attach(Mroz)
GLM.01 <- glm(lfp ~ k5 + k618 + age + wc + hc + lwg + inc, family=binomial(logit), trace=TRUE, data=Mroz)
summary(GLM.01)  #slope is for logit, not for probability
vif(GLM.01)
confint(GLM.01, level=0.95, type="Wald",trace = FALSE)
# confint(GLM.01, level=0.95, type="Wald",trace = FALSE)
library(effects)     # Important: Use version 3.0-6 or newer
plot(allEffects(GLM.01), type="response", ylim=c(0,1), ask=FALSE)
GLM.02 <- glm(lfp ~ k5 + age + wc + lwg + inc, family=binomial(logit), trace=TRUE, data=Mroz)
summary(GLM.02)  #slope is for logit, not for probability
## Likelihood Ratio Test
anova(GLM.02,GLM.01,test = 'LRT')
# Low prob respondent
eff.GLM.low <- effect("inc",GLM.02,
given.values = c(k5 = 2,age = 49,"wcyes" = 0,lwg = 0.81))
plot(eff.GLM.low, type="response", ylim=c(0,1), ylab=expression(Pr(Y[i]=="Close")),
main="Low Probability Respondents")
# High prob respondent
eff.GLM.hi <- effect("inc",GLM.02,
given.values=c(k5 = 0,age = 49,"wcyes" = 1,lwg = 1.40))
plot(eff.GLM.hi, type="response", ylim=c(0,1), ylab=expression(Pr(Y[i]=="Close")),
main="High Probability Respondents")
# High prob respondent
eff.GLM.hi <- effect("inc",GLM.02,
given.values=c(k5 = 0,age = 36,"wcyes" = 1,lwg = 1.40))
plot(eff.GLM.hi, type="response", ylim=c(0,1), ylab=expression(Pr(Y[i]=="Close")),
main="High Probability Respondents")
library(CancerSEA)
install.packages("CancerSEA")
install.packages("CancerSEA_0.9.6.tar.gz", repos=NULL)
library(CancerSEA)
data(cancer)
attach(cancer)
lm.poisson <- glm(L_WM_P2_CN~URBRUR+RAD_MD+I(RAD_MD^2)+TOBACCO, offset=log(L_WM_P2_EX),family=poisson, data=cancer)
summary(lm.poisson)
plot(allEffects(lm.poisson))
cancer$L_WM_P2_RT_rate <- cancer$L_WM_P2_RT/100000
hist(cancer$L_WM_P2_RT_rate)
GLM.01 <- glm(L_WM_P2_RT_rate~ URBRUR+RAD_MD+I(RAD_MD^2)+TOBACCO, family=binomial(logit), weights = (POP1980/2),trace=TRUE, data=cancer)
summary(GLM.01)  #slope is for logit, not for probability
plot(allEffects(GLM.01), type="response", ylim=c(0.0005,0.0009))
GLM.02 <- glm(L_WM_P2_RT_rate~ URBRUR+RAD_MD+I(RAD_MD^2)+TOBACCO, family=quasibinomial, weights = (POP1980/2),trace=TRUE, data=cancer)
summary(GLM.02)  #slope is for logit, not for probability
upfing1 <- foreign::read.spss("UPFING.SAV", use.value.labels=TRUE, to.data.frame=TRUE)
mod01 <- glm(MIJ~log(PI)+log(PJ)+log(DIJ), data=upfing, family=poisson,subset=(I!=J))
upfing <- foreign::read.spss("UPFING.SAV", use.value.labels=TRUE, to.data.frame=TRUE)
mod01 <- glm(MIJ~log(PI)+log(PJ)+log(DIJ), data=upfing, family=poisson,subset=(I!=J))
summary(mod01)
library(ggplot2);library(e1071); library(ISLR) ;library(gmodels); library(ROCR); library(caret);library(parallel)
cl <- parallel::makeCluster(detectCores())
set.seed(100)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <-  1*( x1^2-x2^2 > 0)
plot(x1,x2,col = as.factor(y))
logistic.mod1 <- glm(y ~ ., family = binomial(link="logit"), data = df_train)
summary(logistic.mod1)
logistic_as_factor <- function(result){
return(as.factor(ifelse(result > 0.5,1,0)))
}
logistic.pred <- predict(logistic.mod1,newdata = df_test,type = "response")
logistic.pred <- logistic_as_factor(logistic.pred)
confusionMatrix(logistic.pred,df_test$y)
train_pred <- fitted(logistic.mod1)
train_pred <- logistic_as_factor(train_pred)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_pred, color = train_pred), size = 3, alpha = 0.75)
x1_quad <- df_train$x1^2
x2_quad <- df_train$x2^2
x1.x2 <- df_train$x1 * df_train$x2
logistic.mod2 <- glm(y ~ . + x1_quad + x1.x2 + x2_quad, family = binomial(link="logit"), data = df_train)
summary(logistic.mod2)
df_test$x1_quad <- df_test$x1^2
df_test$x1.x2 <- df_test$x1 * df_test$x2
df_test$x2_quad <- df_test$x2^2
logistic.pred <- predict(logistic.mod2,newdata = df_test,type = "response")
logistic.pred <- logistic_as_factor(logistic.pred)
confusionMatrix(logistic.pred,df_test$y)
train_pred <- fitted(logistic.mod2)
train_pred <- logistic_as_factor(train_pred)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_pred, color = train_pred), size = 3, alpha = 0.75)
set.seed(1)
tune.out <- tune(svm, y~.,data=df_train,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
ypred <- predict(bestmod, df_test)
confusionMatrix(ypred, df_test$y)
train_y <- fitted(bestmod)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_y, color = train_y), size = 3, alpha = 0.75)
set.seed(1)
tune.out <- tune(svm, y~., data=df_train, kernel="radial",
ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out$best.model)
ypred <- predict(tune.out$best.model, df_test)
confusionMatrix(ypred, df_test$y)
train_pred <- fitted(tune.out$best.model)
ggplot(df_train, aes(x = x1, y = x2)) +
geom_point(aes(shape = train_pred, color = train_pred), size = 3, alpha = 0.75)
credit <- read.csv('credit.csv')
index <- createDataPartition(credit$default, p = 0.7, list = FALSE)
df_train <- credit[index, ]
df_test  <- credit[-index, ]
credit <- read.csv('credit.csv')
index <- createDataPartition(credit$default, p = 0.7, list = FALSE)
df_train <- credit[index, ]
df_test  <- credit[-index, ]
# Tune an SVM with radial basis kernel
set.seed(1)  # for reproducibility
credit_svm <- train(
default ~ .,
data = df_train,
method = "svmRadial",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# Plot results
ggplot(credit_svm) + theme_light()
summary(credit_svm$bestTune)
# Control params for SVM
ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary  # also needed for AUC/ROC
)
# Tune an SVM
set.seed(1)  # for reproducibility
credit_svm_auc <- train(
default ~ .,
data = df_train,
method = "svmRadial",
preProcess = c("center", "scale"),
metric = "ROC",  # area under ROC curve (AUC)
trControl = ctrl,
tuneLength = 10
)
# Print results
credit_svm_auc$results
confusionMatrix(credit_svm_auc)
?createDataPartition
credit <- read.csv('credit.csv')
creditSplit <- initial_split(credit, breaks=7, prop=0.7, strata="default")
library(AmesHousing); library(rsample);library(recipes)
credit <- read.csv('credit.csv')
creditSplit <- initial_split(credit, breaks=7, prop=0.7, strata="default")
df_train <- training(creditSplit)
df_test <- testing(creditSplit)
# Tune an SVM with radial basis kernel
set.seed(1)  # for reproducibility
credit_svm <- train(
default ~ .,
data = df_train,
method = "svmRadial",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneLength = 10
)
# Plot results
ggplot(credit_svm) + theme_light()
summary(credit_svm$bestTune)
# Control params for SVM
ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary  # also needed for AUC/ROC
)
# Tune an SVM
set.seed(1)  # for reproducibility
credit_svm_auc <- train(
default ~ .,
data = df_train,
method = "svmRadial",
preProcess = c("center", "scale"),
metric = "ROC",  # area under ROC curve (AUC)
trControl = ctrl,
tuneLength = 10
)
# Print results
credit_svm_auc$results
str(credit_svm_auc)
credit_svm_auc
credit_svm_auc$bestTune
credit_svm_auc$method
credit_svm_auc
credit_svm_auc$results
credit_svm_auc$pred
predict(credit_svm_auc,df_test)
confusionMatrix(predict(credit_svm_auc,df_test),df_test$default)
df_test$default
confusionMatrix(predict(credit_svm_auc,df_test),as.factor(df_test$default))
red_default_prob <- predict(credit_svm_auc,df_test,type = "vector")
red_default_prob <- predict(credit_svm_auc,df_test,type = "prob")
red_default_prob <- predict(credit_svm_auc,df_test,type = "prob")
roc_1 <- roc(as.factor(df_test$default), pred_default_prob[,"yes"])
library(pROC)
red_default_prob <- predict(credit_svm_auc,df_test,type = "prob")
roc_1 <- roc(as.factor(df_test$default), pred_default_prob[,"yes"])
library(pROC)
pred_default_prob <- predict(credit_svm_auc,df_test,type = "prob")
roc_1 <- roc(as.factor(df_test$default), pred_default_prob[,"yes"])
plot(roc_1, col="red", lwd=2)
title(main = paste('Area under the curve: ',auc(roc_1)))
library(pROC)
pred_default_prob <- predict(credit_svm_auc,df_test,type = "prob")
roc_1 <- roc(as.factor(df_test$default), pred_default_prob[,"yes"])
plot(roc_1, col="red", lwd=2)
title(main = paste('Area under the curve: ',auc(roc_1)))
# Control params for SVM
ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary  # also needed for AUC/ROC
)
# Tune an SVM
set.seed(1)  # for reproducibility
credit_svm_auc <- train(
default ~ .,
data = df_train,
method = "svmRadial",
preProcess = c("center", "scale"),
metric = "ROC",  # area under ROC curve (AUC)
trControl = ctrl,
tuneLength = 10
)
credit_svm_auc$bestTune
confusionMatrix(predict(credit_svm_auc,df_test),as.factor(df_test$default))
library(pROC)
pred_default_prob <- predict(credit_svm_auc,df_test,type = "prob")
roc_1 <- roc(as.factor(df_test$default), pred_default_prob[,"yes"])
plot(roc_1, col="red", lwd=2)
title(main = paste('Area under the curve: ',auc(roc_1)))
ames <- make_ames()
# Stratified Random Sampling
amesSplit <- initial_split(ames, breaks=8, prop=0.8, strata="Sale_Price")
ames_train <- training(amesSplit)
ames_test <- testing(amesSplit)
set.seed(1)
ames_svm <- kernlab::ksvm(Sale_Price ~ .,data = ames_train,kernel = 'rbfdot',kpar = 'automatic',
type = 'eps-svr',epsilon = 0.1)
# ames_svm <- train(
#   Sale_Price ~ .,
#   data = ames_train,
#   method = "svmRadial",
#   preProcess = c("center", "scale"),
#   trControl = trainControl(method = "cv", number = 5),
#   tuneLength = 10
# )
# ggplot(ames_svm) + theme_light()
ames_svm
y.pred <- predict(ames_svm)
y.pred <- predict(ames_svm,newdata = ames_test)
set.seed(1)
ames_svm <- kernlab::ksvm(Sale_Price ~ .,data = ames_train,kernel = 'rbfdot',kpar = 'automatic',
type = 'eps-svr',epsilon = 0.1)
# ames_svm <- train(
#   Sale_Price ~ .,
#   data = ames_train,
#   method = "svmRadial",
#   preProcess = c("center", "scale"),
#   trControl = trainControl(method = "cv", number = 5),
#   tuneLength = 10
# )
# ggplot(ames_svm) + theme_light()
ames_svm
y.pred <- predict(ames_svm,newdata = ames_test)
predict(ames_svm)
??ksvm
ames <- make_ames()
# Stratified Random Sampling
amesSplit <- initial_split(ames, breaks=8, prop=0.8, strata="Sale_Price")
ames_train <- training(amesSplit)
ames_test <- testing(amesSplit)
# Data preprocessing
# blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
#   step_nzv(all_nominal())  %>%
#   step_integer(matches("Qual|Cond|QC|Qu")) %>%
#   step_center(all_numeric(), -all_outcomes()) %>%
#   step_scale(all_numeric(), -all_outcomes()) %>%
#   step_pca(all_numeric(), -all_outcomes())
# prepare <- prep(blueprint, training = ames_train)
# baked_train <- bake(prepare, new_data = ames_train)
# baked_test <- bake(prepare, new_data = ames_test)
# # baked_train
set.seed(1)
ames_svm <- kernlab::ksvm(Sale_Price ~ .,data = ames_train,kernel = 'rbfdot',kpar = 'automatic',
type = 'eps-svr',epsilon = 0.1)
# ames_svm <- train(
#   Sale_Price ~ .,
#   data = ames_train,
#   method = "svmRadial",
#   preProcess = c("center", "scale"),
#   trControl = trainControl(method = "cv", number = 5),
#   tuneLength = 10
# )
# ggplot(ames_svm) + theme_light()
set.seed(1)
ames_svm <- kernlab::ksvm(Sale_Price ~ .,data = ames_train,kernel = 'rbfdot',kpar = 'automatic',
type = 'eps-svr',epsilon = 0.1)
# ames_svm <- train(
#   Sale_Price ~ .,
#   data = ames_train,
#   method = "svmRadial",
#   preProcess = c("center", "scale"),
#   trControl = trainControl(method = "cv", number = 5),
#   tuneLength = 10
# )
# ggplot(ames_svm) + theme_light()
y.pred <- predict(ames_svm,newdata = ames_test)
library(kernlab)
library(kernlab)
set.seed(1)
ames_svm <- ksvm(Sale_Price ~ .,data = ames_train,kernel = 'rbfdot',kpar = 'automatic',
type = 'eps-svr',epsilon = 0.1)
# ames_svm <- train(
#   Sale_Price ~ .,
#   data = ames_train,
#   method = "svmRadial",
#   preProcess = c("center", "scale"),
#   trControl = trainControl(method = "cv", number = 5),
#   tuneLength = 10
# )
# ggplot(ames_svm) + theme_light()
y.pred <- predict(ames_svm,newdata = ames_test)
svm_result <- data.frame(predict = y.pred, observed = ames_test$Sale_Price)
ggplot(svm_result,aes(x=predict, y=observed)) +
geom_point() +
geom_smooth(method=lm, se=FALSE, fullrange=TRUE)
#install.packages("neuralnet")
RNGversion("3.5.3"); set.seed(12345)
library(neuralnet)
help("neuralnet")
## custom normalization function
normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }
## create the custom soft-plus activation function
softplus <- function(x) { log(1 + exp(x)) }
data(Boston, package="MASS")
summary(Boston)
# ## Exclude home values that were trunctated at $50k
# sel <- Boston$medv == 50
# table(sel)
# Boston <- Boston[!sel,]
## apply normalization to entire data frame
nBoston <- as.data.frame(lapply(Boston, normalize))
##
## Investigation of model structure without split into training and test sample
##
## Logistic Regression possible here because y transformed into (0,1).
## This is just for demonstration purposes. It should NOT be done in practice!
## Significance not relevant here
logit01 <- glm(medv ~ ., data = nBoston, family=binomial())
summary(logit01)
car::vif(logit01)
logit01.pred <- predict(logit01, type="response")
cor(logit01.pred, nBoston$medv)
## Neural Network Model
nnet01 <- neuralnet(medv ~ ., data = nBoston, act.fct="logistic",
linear.output = TRUE, stepmax=10000, hidden = c(1))
## plot the network with weights and bias terms
plot(nnet01)
## Neural Network Model
nnet01 <- neuralnet(medv ~ ., data = nBoston, act.fct="logistic",
linear.output = TRUE, stepmax=10000, hidden = c(1))
## plot the network with weights and bias terms
plot(nnet01)
set.seed(12345) # to guarantee repeatable results
K <- 4                                            # number of folds
idx <- sample(1:nrow(nBoston))
folds <- cut(idx, breaks=K, labels=FALSE)
nOfNeurons <- 4:6                                # value range of neuron numbers in first layer
scoreCor <- matrix(0, nrow=length(nOfNeurons), ncol=K)
system.time(
for (j in 1:length(nOfNeurons)){
for (i in 1:K){
cat("processing neurons=",nOfNeurons[j],"for fold #", i, "\n")
valIdx <- which(folds == i, arr.ind=TRUE)
test <- nBoston[valIdx, ]
train <- nBoston[-valIdx, ]
set.seed(12345) # to guarantee repeatable results
medvNN <- neuralnet(medv ~ ., data = train,  linear.output = TRUE,
stepmax=10000000, hidden = nOfNeurons[j], act.fct = softplus)
result <- predict(medvNN, test[1:13])
scoreCor[j,i] <- cor(result, test$medv)
} #end::for_i
} #end::for_j
) #end::system.time
colnames(scoreCor) <- c("Fold 1", "Fold 2", "Fold 3", "Fold 4")
result <- data.frame(nOfNeurons, Average=rowMeans(scoreCor), scoreCor)
## Evaluate fit by number of nodes in a one-layer neural network.
round(result,3)
