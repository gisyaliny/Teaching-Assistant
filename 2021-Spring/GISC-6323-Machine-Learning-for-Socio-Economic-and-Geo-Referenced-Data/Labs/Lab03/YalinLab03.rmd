---
title: "GISC-6323-Machine-Learning-for-Socio-Economic-and-Geo-Referenced-Data Lab03"
author: "Yalin Yang"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    df_print: paged
  pdf_document:
    toc: yes
vignette: |
  %\VignetteIndexEntry{DallasMarketAreas} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---
# Part 1: ROC

## Task 1: Receiver Operating Curve [3 points]
[a] Calculate the sensitivity and specificity at cut-off values $π_{cut-off} =\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}$ for predicted positive probabilities $Pr⁡(i="positive")$. Also draw the associated ROC diagram.The predicted probabilities are given in the table below:
![table1](table01.png)

```{r}
prob <- c(seq(0,0.45,by=0.05),seq(0.55,1,by=0.05))
observation <- as.factor(c(rep("neg",10),rep("pos",10)))
pi <- seq(0.1,0.9,by = 0.1)

get_spec_and_sens <- function(observation,prob,pi){
  Sensitivity <- c()
  Specificity <- c()
  i = 0.1
  for(i in pi){
    pred <- as.factor(ifelse(prob < i, "neg","pos"))
    result  <-  caret::confusionMatrix( pred,observation, positive = "pos")
    Sensitivity <- c(Sensitivity,result$byClass[1])
    Specificity <- c(Specificity,result$byClass[2])
  }
  df <- t(as.data.frame(cbind(Sensitivity,Specificity)))
  colnames(df) <- pi
  return(df)
  
}

result <- get_spec_and_sens(observation,prob,pi)
result

```

[b] Calculate the sensitivity and specificity at cut-off $π_{cut-off} =\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}$ for predicted positive probabilities Pr⁡(i="positive" ). Also draw the associated ROC diagram. The predicted probabilities are given in the table below:
![table2](table02.png)

```{r}
prob2 <- c(0.55,0.05,0.65,0.15,0.75,0.25,0.85,0.35,0.95,0.45,0,0.6,0.1,0.7,0.2,0.8,0.3,0.9,0.4,1)
result2 <- get_spec_and_sens(observation,prob2,pi)
result2
```

[c] Interpret and compare both ROC diagrams with respect to their underlying data in tasks 1 [a] and [b].

Our model 1 identify those two categories quite well, the predicted probabilities of all "positive" class are larger than 0.5, so set $\pi=0.5$ is ideal for this circumstance (both specificity and sensitivity equal to 1). 
For model 2, the predicted probability of both "positive" and "negative" classes are overlapped with each other, which means our model is not doing a good job for classification. We need to make a trade-off between specificity and sensitivity.

```{r message=FALSE, warning=FALSE}
library(pROC)
rocResult1 <- roc(observation, prob)
rocResult2 <- roc(observation, prob2)
plot(rocResult1, col="red", lwd=2)
plot(rocResult2, col="blue", lwd=2, add = TRUE)
legend("bottomright", legend=c("[b]","[a]"), lty=1, col=c("blue","red"))
title(main = paste('Area under the curve [a]:',auc(rocResult1),'; Area under the curve [b]:',auc(rocResult2)))
```

# Part 2: Trees

For the following tasks continue working with the `credit.csv` data set to predict the default probabilities. Split the data into a stratified training data set with 70% of the observations and a test data set with the remaining 30% of the observations.


## Task 2: Standard Tree [2 points]

```{r message=FALSE, warning=FALSE}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting

# Modeling packages
library(rpart)       # direct engine for decision tree application
library(caret)       # meta engine for decision tree application

# Model interpretability packages
library(rpart.plot)  # for plotting decision trees
library(vip)         # for feature importance
library(pdp)         # for feature effects
```


Build tree with the control parameters `mincut=5` and `minsize=10` based of the training data set. Plot the tree. Evaluate the predictive quality of this tree using the test data set with respect to the `confusionMatrix`, `auc` and the `roc` functions. Evaluate the importance of the individual features and whether particular features were not included in building the tree.

```{r}
?read.csv
df <- read.csv('credit.csv',stringsAsFactors = TRUE)
str(df)
```


```{r fig.height=6, fig.width=10, message=FALSE, warning=FALSE}
library(tree)
set.seed(123)  # for reproducibility
index <- createDataPartition(df$default, p = 0.7, list = FALSE)
train <- df[index, ]
test  <- df[-index, ]
my_tree <- tree(default~., data=train,split = "deviance",control=tree.control(nobs = nrow(train),minsize=10, mincut=5))
```


```{r}
summary(my_tree)
```

We could know that the `checking balance`, `moths loan duration` and `other credit` are the top 3 important factors for our predicted result from the following graphs. And `years_at_residence`,`percent_of_income `,`other_credit` are not included in our model.

```{r fig.height=6, fig.width=10}
## Feature Interpretation
plot(my_tree); text(my_tree, pretty=0)
```

```{r}
# library(dplyr)       # for data wrangling
# library(ggplot2)     # for awesome plotting
# library(ROCR)
# library(RColorBrewer)
# 
# # Modeling packages
# library(rpart)       # direct engine for decision tree application
# library(caret)       # meta engine for decision tree application
# library(rattle)
# library(ranger)
# library(gbm)
# 
# # Model interpretability packages
# library(rpart.plot)  # for plotting decision trees
# library(vip)         # for feature importance
# library(pdp)         # for feature effects
# library(ipred)


```

```{r}
pred_default <- predict(my_tree, newdata=test, type="class")

caret::confusionMatrix(as.factor(test$default), pred_default, 
                       positive = "yes",dnn = c("Reference","Prediction"))

(confMat <- table(test$default, pred_default))
predicted_probability = predict(my_tree, test, type = "vector")[,2]
```

```{r message=FALSE, warning=FALSE}
pred_default_prob <- predict(my_tree, newdata=test, type="vector")
roc_tree1 <- roc(as.factor(test$default), pred_default_prob[,"yes"])
plot(roc_tree1, col="red", lwd=2)
title(main = paste('Area under the curve: ',auc(roc_tree1)))
```

## Pruned Tree [2 points]

Identify an optimally pruned the tree using cross-evaluation based of the training data set. Plot the final tree Evaluate the predictive quality of this pruned tree using the test data set with respect to the `confusionMatrix`, `auc` and the `roc` functions. Evaluate the importance of the individual features.

```{r fig.height=5, fig.width=12}

set.seed(123)
cv.carseats <- cv.tree(my_tree, FUN=prune.misclass, K=10)
par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b", main="Deviance")
plot(cv.carseats$k, cv.carseats$dev, type="b", main="Complexity Parameter")
```
```{r fig.height=6, fig.width=12}
pruned_tree <- prune.misclass(my_tree, best=6)
plot(pruned_tree); text(pruned_tree, pretty=0)
```

We could know that the checking balance and months loan duration are the top 2 important factors for our predicted result from this graph. If one have enough checking balance, he would not default. If the monthly loan duration less than 2 years, he has a lower probability of default. If not, a higher probability.

The overall accuracy is the same as the standard tree.

```{r}
pred_default2 <- predict(pruned_tree, newdata=test, type="class")
caret::confusionMatrix(as.factor(test$default), pred_default2, 
                       positive = "yes",dnn = c("Reference","Prediction"))
```

```{r message=FALSE, warning=FALSE}
pred_default2_prob <- predict(pruned_tree, newdata=test, type="vector")
roc_tree2 <- roc(as.factor(test$default), pred_default2_prob[,"yes"])
plot(roc_tree2, col="red", lwd=2)
title(main = paste('Area under the curve: ',auc(roc_tree2)))
```

## Bootstrapped (Bagged) Tree [2 points]

Identify an optimally bootstrapped tree based of the training data set. Evaluate the predictive quality of this bootstrapped tree using the test data set with respect to the `confusionMatrix`, `auc` and the `roc` functions. Evaluate the importance of the individual features.

```{r}

```




