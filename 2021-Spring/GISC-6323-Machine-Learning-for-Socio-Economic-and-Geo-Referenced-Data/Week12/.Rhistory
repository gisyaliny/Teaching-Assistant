plot(close~female, data=CloseVote)
plot(close~kids, data=CloseVote)
plot(close~nodad, data=CloseVote)
##
## Full model with all variables and interaction term nodad
##
GLM.02 <- glm(close ~ lived + educ + contam + hsc + nodad + female + kids ,
family=binomial(logit), trace=TRUE, data=CloseVote)
##
## Exploratory plots: barwidth _proportional_ to the NumOfObs in interval
##
plot(close~lived, data=CloseVote)
plot(close~educ, data=CloseVote)
plot(close~contam, data=CloseVote)
plot(close~hsc, data=CloseVote)
plot(close~female, data=CloseVote)
plot(close~kids, data=CloseVote)
plot(close~nodad, data=CloseVote)
plot(close~female, data=CloseVote)
plot(close~contam, data=CloseVote)
plot(close~hsc, data=CloseVote)
##
## Full model with all variables and interaction term nodad
##
GLM.02 <- glm(close ~ lived + educ + contam + hsc + nodad + female + kids ,
family=binomial(logit), trace=TRUE, data=CloseVote)
summary(GLM.02, correlation=F)
logLik(GLM.02)
confint(GLM.02, level=0.95, type="Wald")
model.matrix(GLM.02)     # nodad is an interaction term
##
## Restricted model without main-effects: female and kids
##
GLM.03 <- glm(close ~ lived + educ + contam + hsc + nodad ,
family=binomial(logit), data=CloseVote,
control=list(epsilon=1e-15,maxit=50, trace=TRUE))
summary(GLM.03, correlation=TRUE)
logLik(GLM.03)
summary(GLM.03, correlation=FALSE)
## Likelihood Ratio Test
( LR <- -2*(logLik(GLM.03)-logLik(GLM.02)) )
( pchisq(LR[1], df=2, lower.tail=F) )
anova(GLM.03,GLM.02,test="LRT")
##
## Effect plots (note: book uses average educ=12.95 for all models)
##
library(effects)     # Important: Use version 3.0-6 or newer
## all independent variables with the others at average.
## Note type="response" for probability scale
plot(allEffects(GLM.03), type="response", ylim=c(0,1), ask=FALSE)
## Group specifice effect plots
summary(CloseVote)
# Low prob respondent
eff.GLM.low <- effect("lived",GLM.03,
given.values=c(educ=20,"contamyes"=0,"hscyes"=0,"nodadyes"=1))
plot(eff.GLM.low, type="response", ylim=c(0,1), ylab=expression(Pr(Y[i]=="Close")),
main="Low Probability Respondents")
plot(eff.GLM.low, type="response", ylim=c(0,1), ylab=expression(Pr(Y[i]=="Close")),
main="Low Probability Respondents")
# Average prob respondent
eff.GLM.average <- effect("lived",GLM.03)
plot(eff.GLM.average, ylim=c(0,1), type="response", ylab=expression(Pr(Y[i]=="Close")), # ylim is in terms of probs
main="Average Probability Respondents")
# High prob respondent
eff.GLM.hi <- effect("lived",GLM.03,
given.values=c(educ=6,"contamyes"=1,"hscyes"=1,"nodadyes"=0))
plot(eff.GLM.hi, type="response", ylim=c(0,1), ylab=expression(Pr(Y[i]=="Close")),
main="High Probability Respondents")
##
## High Discrimination. Quasi-complete separation
##
(tab <- xtabs(~close+nodad, data=CloseVote))
##
## High Discrimination. Quasi-complete separation
##
(tab <- xtabs(~close+nodad, data=CloseVote))
RcmdrMisc::colPercents(tab) # Column Percentages
chisq.test(tab, correct=FALSE)
## removing the 4 nodad observations in favor of closing
CloseVoteSel <- subset(CloseVote, !(close=="close" & nodad=="yes"))
xtabs(~close+nodad, data=CloseVoteSel)
GLM.04 <- glm(close ~ lived + educ + contam + hsc + nodad ,
family=binomial(logit), data=CloseVoteSel,
control=list(epsilon=1e-15,maxit=50, trace=TRUE))
summary(GLM.04)
GLM.04 <- glm(close ~ lived + educ + contam + hsc + nodad ,
family=binomial(logit), data=CloseVoteSel,
control=list(epsilon=1e-15,maxit=50, trace=TRUE))
summary(GLM.04)
##
## Another high discrimination example
##
y <- c(0,0,0,0,0,1,1,1,1,1)
x <- c(1,2,3,4,5,6,7,8,9,10)
glmHiDis <- glm(y~x, family=binomial(logit), control=list(epsilon=1e-15,maxit=50, trace=TRUE))
summary(glmHiDis)
invlogit <- function(x) 1/(1+exp(-x))   # inverse logit function
plot(x, y, type="n", ylim=c(0,1),
ylab="Prob(Y=1)", xlab="X", main="Conditional Effects Plot")
rug(x[y==0])
rug(x[y==1], side=3)
abline(h=c(0,1),lty=3)
curve(invlogit(cbind(1,x) %*% coef(glmHiDis)), lwd=2, add=TRUE)
setwd("G:/UTD_Classes/Teaching-Assistant/2021-Spring/GISC-7310-Advanced-GIS-Data-Analysis/Week13")
gw <- foreign::read.dbf("Glasgow.dbf")
head(gw)
gw$RATE2010 <- gw$OBSRES2010/gw$POP2001
##
## Logistic regression with rates
##
glm.01 <- glm(RATE2010~INCOMEDEP+TYPE , weight=POP2001, data=gw, family=binomial)
summary(glm.01)
## Account for over-dispersion
glm.02 <- update(glm.01, family=quasibinomial)
summary(glm.02)
##
## Poisson regression with offset
##
glm.03 <- glm(OBSRES2010~INCOMEDEP+TYPE, data=gw, offset=log(EXPRES2010), family=poisson(link="log"))
summary(glm.03)
## Account for over-dispersion
glm.04 <- glm(OBSRES2009~INCOMEDEP+TYPE, data=gw, offset=log(EXPRES2009), family=quasipoisson(link="log"))
summary(glm.04)
berlin <- foreign::read.spss("bmigvec91_92.sav", use.value.labels=TRUE, to.data.frame=TRUE)
attach(berlin)
names(berlin)
##
## Prepare the variables
## index i for origin, index j for destination, and index ij for inter-regional links
##
i <- org
j <- dest
lnpi91 <- log(poporg91)
lnpj91 <- log(popdest91)
lnpi92 <- log(poporg92)
lnpj92 <- log(popdest92)
lnmij91 <- mij91
lnmij91[mij91 >0 ] <- log(mij91[mij91 >0 ])
lndij <- dij                         # Make sure that the log(dij=0) is treated properly
lndij[dij > 0] <- log(dij[dij > 0])  # dij = 0 remains zero
## Plain Migration Model
mod01 <- glm(mij92 ~ lnpi92+lnpj92+lndij, weights = cwt, data=berlin, family = poisson(log))
summary(mod01)
logLik(mod01)
mod01b <- glm(mij92 ~ lnpi92+lnpj92+lndij, weights = cwt, data=berlin, family = quasipoisson)
summary(mod01b)
logLik(mod01b)
RNGversion("3.5.4"); set.seed(12345)
##
## Define loss and gradient search functions
##
lossFct <- function(y,x,b0,b1){
sum((y-(b0+b1*x))^2)
} #end::lossFct
gradientUpdateFct <- function(y,x,b0Old,b1Old,alpha){
# The objective is to move to a minimum loss
derivb0 <- -2*sum((y - (b0Old+b1Old*x)))    # at optimum equal to zero
derivb1 <- -2*sum(x*(y - (b0Old+b1Old*x)))  # at optimum equal to zero
b0New <- b0Old - derivb0*alpha/length(x)
b1New <- b1Old - derivb1*alpha/length(x)
return(c(b0New,b1New))
} # end::gradientUpdateFunction
##
## Simulate linear regression data
##
b0 <- 1; b1 <- 1; n <- 200
x <- runif(n,0,2)
y <- b0 + b1*x + rnorm(n, sd=0.25)
summary(lm(y~x))
plot(y~x, ylim=c(0,4), main="Simulated Regression Data")
abline(lm(y~x), col="red")
##
## Set parameters controlling the gradient search and history
##
alpha <- 0.05                # Learning rate: update step length. Caution: 0.422 leads to a run-away solution
threshold <- 0.00001         # Change in the loss of subsequent iterations
b0 <- 0; b1 <- 0             # Set starting values
loss <- lossFct(y,x,b0,b1)   # loss at start
##
## Start iterative search
##
abline(a=b0, b=b1, col="green")  # Starting regression line
i <- 1
iterHist <- rbind(NULL,c(i,b0,b1,loss))
lossOld <- Inf
while (abs(lossOld - loss) > threshold){
i <- i+1; lossOld <- loss
b <- gradientUpdateFct(y,x,b0,b1,alpha)
b0 <- b[1]; b1 <- b[2]
loss <- lossFct(y,x,b0,b1)
iterHist <- rbind(iterHist,c(i,b0,b1,loss))
if ((i %% 50)==0){                        # plot results of every 50th iteration
abline(a=b0, b=b1, col="green")         # updated regression line
cat(i,b0,b1,loss,"\n")
Sys.sleep(1)
} #end::if
} # end::while
abline(lm(y~x), col="red")                  # plot final regression line again
##
## Setup loss surface in parameter space
##
b0Grid <- seq(0,2, by=0.01)
b1Grid <- seq(0,2, by=0.01)
lossValMat <- matrix(NA, nrow=length(b0Grid), ncol=length(b1Grid))
for (i in 1:length(b0Grid))
for (j in 1:length(b1Grid))
lossValMat[i,j] <- lossFct(y,x,b0Grid[i],b1Grid[j])
##
## Plot loss surface and iteration history
##
image(b0Grid, b1Grid, lossValMat, ,xlab=expression(b[0]),ylab=expression(b[1]),
main="Loss Surface and Iteration History")
abline(v=coef(lm(y~x))[1], h=coef(lm(y~x))[2], lty="longdash")
points(iterHist[1:nrow(iterHist),2:3], pch=".", cex=4, col="green")
abline(a=b0, b=b1, col="green")  # Starting regression line
i <- 1
iterHist <- rbind(NULL,c(i,b0,b1,loss))
lossOld <- Inf
while (abs(lossOld - loss) > threshold){
i <- i+1; lossOld <- loss
b <- gradientUpdateFct(y,x,b0,b1,alpha)
b0 <- b[1]; b1 <- b[2]
loss <- lossFct(y,x,b0,b1)
iterHist <- rbind(iterHist,c(i,b0,b1,loss))
if ((i %% 50)==0){                        # plot results of every 50th iteration
abline(a=b0, b=b1, col="green")         # updated regression line
cat(i,b0,b1,loss,"\n")
Sys.sleep(1)
} #end::if
} # end::while
# setwd("E:\\Lectures2021\\GISC6323\\Week12")
library(neuralnet)
help(neuralnet)
concrete <- read.csv("concrete.csv")
setwd("G:/UTD_Classes/Teaching-Assistant/2021-Spring/GISC-6323-Machine-Learning-for-Socio-Economic-and-Geo-Referenced-Data/Week12")
concrete <- read.csv("concrete.csv")
concrete <- read.csv("concrete.csv")
summary(concrete)
help(neuralnet)
# apply normalization to entire data frame
concrete_norm <- as.data.frame(lapply(concrete, normalize))
# create training and test data
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
# create a custom softplus activation function. Similar to rectified linear unit function
softplus <- function(x) { log(1 + exp(x)) }
curve(softplus, from=-5, to=5); abline(v=0, h=0, lty=3)
set.seed(12345) # to guarantee repeatable results
concrete_model3 <- neuralnet(strength ~ cement + slag + ash + water + superplastic +
coarseagg + fineagg + age, data = concrete_train,
threshold = 0.01, stepmax=1000000,
linear.output = TRUE, hidden = c(5), act.fct = softplus)
# create training and test data
concrete_train <- concrete_norm[1:773, ]
# apply normalization to entire data frame
concrete_norm <- as.data.frame(lapply(concrete, normalize))
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply normalization to entire data frame
concrete_norm <- as.data.frame(lapply(concrete, normalize))
# create training and test data
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
# create a custom softplus activation function. Similar to rectified linear unit function
softplus <- function(x) { log(1 + exp(x)) }
curve(softplus, from=-5, to=5); abline(v=0, h=0, lty=3)
set.seed(12345) # to guarantee repeatable results
concrete_model3 <- neuralnet(strength ~ cement + slag + ash + water + superplastic +
coarseagg + fineagg + age, data = concrete_train,
threshold = 0.01, stepmax=1000000,
linear.output = TRUE, hidden = c(5), act.fct = softplus)
# plot the network
plot(concrete_model3)
# evaluate the results as we did before
model_results3 <- predict(concrete_model3, concrete_test[1:8])
plot(model_results3~concrete_test$strength)
cor(model_results3, concrete_test$strength)
help(neuralnet)
# plot the network
plot(concrete_model3)
# evaluate the results as we did before
model_results3 <- predict(concrete_model3, concrete_test[1:8])
plot(model_results3~concrete_test$strength)
cor(model_results3, concrete_test$strength)
# note that the predicted and actual values are on different scales
strengths <- data.frame(
actual = concrete$strength[774:1030],
pred = model_results3
)
# create an unnormalize function to reverse the normalization
unnormalize <- function(x) {
return((x * (max(concrete$strength)) -
min(concrete$strength)) + min(concrete$strength))
}
strengths$pred_new <- unnormalize(strengths$pred)
strengths$error <- strengths$pred_new - strengths$actual
head(strengths, n = 6)
plot(strengths$pred_new~strengths$actual)
cor(strengths$pred_new, strengths$actual)
hist(strengths$error/strengths$actual)  # Don't trust the predictions to build bridges
##
## K-fold cross-validation for small sample sizes
##
set.seed(12345) # to guarantee repeatable results
cor(strengths$pred_new, strengths$actual)
##
## K-fold cross-validation for small sample sizes
##
set.seed(12345) # to guarantee repeatable results
K <- 5                                     # number of folds
idx <- sample(1:nrow(concrete))
folds <- cut(idx, breaks=K, labels=FALSE)
library(e1071); library(ISLR) ;library(gmodels); library(ROCR); library(caret)
##
## Support Vector Classifier
##
RNGversion("3.5.1"); set.seed(1)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
plot(x[,1]~x[,2], col=(3-y), cex=1.5, pch=19)
df <- data.frame(x=x, y=as.factor(y))
help(svm)
svmfit <- svm(y~., data=df, kernel="linear", cost=10, scale=FALSE)
plot(svmfit, df)
svmfit$index           # Observations used as support vectors
summary(svmfit)
svmfit <- svm(y~., data=df, kernel="linear", cost=0.1, scale=FALSE)
plot(svmfit, df)
svmfit$index
summary(svmfit)
## Find best cost parameter
set.seed(1)
tune.out <- tune(svm, y~.,data=df,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
## Set up test data
xtest <- matrix(rnorm(20*2), ncol=2)
ytest <- sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,] <- xtest[ytest==1,] + 1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
ypred <- predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
## Change cost value
svmfit <- svm(y~., data=df, kernel="linear", cost=0.01, scale=FALSE)
ypred <- predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)
## Define linearly separable training sample by increasing the seperation
x[y==1,] <- x[y==1,]+0.5
plot(x[,1]~x[,2], col=(y+5)/2, cex=1.5, pch=19)
df <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=df, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit, df)
svmfit=svm(y~., data=df, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit,df)
##
## Support Vector Machine
##
set.seed(1)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,] <- x[1:100,]+2
x[101:150,] <- x[101:150,]-2
y <- c(rep(1,150),rep(2,50))
df <- data.frame(x=x, y=as.factor(y))
plot(x[,1]~x[,2], col=y)
train <- sample(1:200, 100)
svmfit <- svm(y~., data=df[train,], kernel="radial",  gamma=1, cost=1)
plot(svmfit, df[train,])
#install.packages("neuralnet")
RNGversion("3.5.3"); set.seed(12345)
library(neuralnet)
help("neuralnet")
## custom normalization function
normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }
## create the custom soft-plus activation function
softplus <- function(x) { log(1 + exp(x)) }
data(Boston, package="MASS")
summary(Boston)
# ## Exclude home values that were trunctated at $50k
# sel <- Boston$medv == 50
# table(sel)
# Boston <- Boston[!sel,]
## apply normalization to entire data frame
nBoston <- as.data.frame(lapply(Boston, normalize))
##
## Investigation of model structure without split into training and test sample
##
## Logistic Regression possible here because y transformed into (0,1).
## This is just for demonstration purposes. It should NOT be done in practice!
## Significance not relevant here
logit01 <- glm(medv ~ ., data = nBoston, family=binomial())
summary(logit01)
car::vif(logit01)
logit01.pred <- predict(logit01, type="response")
cor(logit01.pred, nBoston$medv)
## Neural Network Model
nnet01 <- neuralnet(medv ~ ., data = nBoston, act.fct="logistic",
linear.output = TRUE, stepmax=10000, hidden = c(1))
## plot the network with weights and bias terms
plot(nnet01)
#install.packages("neuralnet")
RNGversion("3.5.3"); set.seed(12345)
library(neuralnet)
help("neuralnet")
## custom normalization function
normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }
## create the custom soft-plus activation function
softplus <- function(x) { log(1 + exp(x)) }
data(Boston, package="MASS")
summary(Boston)
# ## Exclude home values that were trunctated at $50k
# sel <- Boston$medv == 50
# table(sel)
# Boston <- Boston[!sel,]
## apply normalization to entire data frame
nBoston <- as.data.frame(lapply(Boston, normalize))
##
## Investigation of model structure without split into training and test sample
##
## Logistic Regression possible here because y transformed into (0,1).
## This is just for demonstration purposes. It should NOT be done in practice!
## Significance not relevant here
logit01 <- glm(medv ~ ., data = nBoston, family=binomial())
summary(logit01)
car::vif(logit01)
logit01.pred <- predict(logit01, type="response")
cor(logit01.pred, nBoston$medv)
## Neural Network Model
nnet01 <- neuralnet(medv ~ ., data = nBoston, act.fct="logistic",
linear.output = TRUE, stepmax=10000, hidden = c(1))
## plot the network with weights and bias terms
plot(nnet01)
##
## Compare logistic parameters to NN weights.
## Note intercept and bias coefficients are different.
##
str(nnet01)
data.frame(Logistic=coef(logit01), NeuroNet=nnet01$weights[[1]][[1]])
plot(coef(logit01)~nnet01$weights[[1]][[1]],
xlab="Neural Network", ylab="Logistic",
main="NN Weights against Logistic Coefficients")
text(nnet01$weights[[1]][[1]], coef(logit01), names(coef(logit01)))
abline(h=0, v=0, lty=3)                       # Reference frame
abline(a=0, b=1, lty=5)                       # Identity line
##
## Compare logistic parameters to NN weights.
## Note intercept and bias coefficients are different.
##
# str(nnet01)
data.frame(Logistic=coef(logit01), NeuroNet=nnet01$weights[[1]][[1]])
plot(coef(logit01)~nnet01$weights[[1]][[1]],
xlab="Neural Network", ylab="Logistic",
main="NN Weights against Logistic Coefficients")
text(nnet01$weights[[1]][[1]], coef(logit01), names(coef(logit01)))
abline(h=0, v=0, lty=3)                       # Reference frame
abline(a=0, b=1, lty=5)                       # Identity line
##
## Compare logistic parameters to NN weights.
## Note intercept and bias coefficients are different.
##
# str(nnet01)
data.frame(Logistic=coef(logit01), NeuroNet=nnet01$weights[[1]][[1]])
plot(coef(logit01)~nnet01$weights[[1]][[1]],
xlab="Neural Network", ylab="Logistic",
main="NN Weights against Logistic Coefficients")
text(nnet01$weights[[1]][[1]], coef(logit01), names(coef(logit01)))
abline(h=0, v=0, lty=3)                       # Reference frame
abline(a=0, b=1, lty=5)                       # Identity line
## evaluate the prediction
nnet01.pred <- predict(nnet01, nBoston[,1:13])
plot(nnet01.pred~nBoston$medv)
cor(nnet01.pred,nBoston$medv)
## back to original scale
medv.pred <- nnet01.pred * (max(Boston$medv)-min(Boston$medv)) + min(Boston$medv)
medv.error <- Boston$medv - medv.pred
hist(medv.error/Boston$medv)  # Error between -120% and 50% of the original median value
set.seed(12345) # to guarantee repeatable results
K <- 4                                            # number of folds
idx <- sample(1:nrow(nBoston))
folds <- cut(idx, breaks=K, labels=FALSE)
nOfNeurons <- 4:6                                # value range of neuron numbers in first layer
scoreCor <- matrix(0, nrow=length(nOfNeurons), ncol=K)
set.seed(12345) # to guarantee repeatable results
K <- 4                                            # number of folds
idx <- sample(1:nrow(nBoston))
folds <- cut(idx, breaks=K, labels=FALSE)
nOfNeurons <- 4:6                                # value range of neuron numbers in first layer
scoreCor <- matrix(0, nrow=length(nOfNeurons), ncol=K)
system.time(
for (j in 1:length(nOfNeurons)){
for (i in 1:K){
cat("processing neurons=",nOfNeurons[j],"for fold #", i, "\n")
valIdx <- which(folds == i, arr.ind=TRUE)
test <- nBoston[valIdx, ]
train <- nBoston[-valIdx, ]
set.seed(12345) # to guarantee repeatable results
medvNN <- neuralnet(medv ~ ., data = train,  linear.output = TRUE,
stepmax=10000000, hidden = nOfNeurons[j], act.fct = softplus)
result <- predict(medvNN, test[1:13])
scoreCor[j,i] <- cor(result, test$medv)
} #end::for_i
} #end::for_j
) #end::system.time
colnames(scoreCor) <- c("Fold 1", "Fold 2", "Fold 3", "Fold 4")
result <- data.frame(nOfNeurons, Average=rowMeans(scoreCor), scoreCor)
## Evaluate fit by number of nodes in a one-layer neural network.
round(result,3)
