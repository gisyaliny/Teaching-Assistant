City <- rgdal::readOGR(dsn="./MyStudyArea/MyPopPlaces.shp",integer64="warn.loss")
CityDf <- as.data.frame(City)
polyCol <- c("orange1","orange4","orange3","orange2")
plot(Shape, border="grey", col=polyCol)
box()
wgt <- (CityDf[,"POPULATION"]/max(CityDf[,"POPULATION"]))
wgt <- sqrt(wgt/pi)*6
n <- nrow(CityDf)
points(City,pch=16,col="blue",cex=wgt)
CityDf$REGIONID
## Arithmetic means
MeanX <- tapply(CityDf[,"coords.x1"],CityDf[,"REGIONID"],mean,simplify=F)
MeanY <- tapply(CityDf[,"coords.x2"],CityDf[,"REGIONID"],mean,simplify=F)
(cbind(MeanX,MeanY))
points(MeanX,MeanY,col="blue",pch=7,cex=1.5)
regionSet <- unique(CityDf[,"REGIONID"])
# for (idxReg in regionSet){                                          # cycle over regions
#   City.id <- which(CityDf[,"REGIONID"] == idxReg)                     # get record IDs for particular region
#   ## Weighted means (tapply does not work for function "weighted.mean)
#   x.w.mean <- weighted.mean(CityDf[City.id,4],CityDf[City.id,"POPULATION"],na.rm=TRUE)
#   y.w.mean <- weighted.mean(CityDf[City.id,5],CityDf[City.id,"POPULATION"],na.rm=TRUE)
#   points(x.w.mean,y.w.mean,col="red",pch=11,cex=1.5)                   # Marked weighted mean by cross
# }
library(foreach)
df1 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
## Weighted means (tapply does not work for function "weighted.mean)
x.w.mean <- weighted.mean(CityDf[City.id,4],CityDf[City.id,"POPULATION"],na.rm=TRUE)
y.w.mean <- weighted.mean(CityDf[City.id,5],CityDf[City.id,"POPULATION"],na.rm=TRUE)
points(x.w.mean,y.w.mean,col="red",pch=11,cex=1.5)
data.frame(x.w.mean,y.w.mean)
}
euclidMedian <- function(xy){
## Function that evaluates the cost at any point XY given
## the global vector argments: x, y, and w
cost <- sum(w * sqrt( (x-xy[1])^2 + (y-xy[2])^2 ))
return(cost)
}
df2 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
## Weighted means (tapply does not work for function "weighted.mean)
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xWMean <- sum(w*x)/sum(w)
yWMean <- sum(w*y)/sum(w)
iIter <- 0
xOld <- xWMean
yOld <- yWMean
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
cost <- euclidMedian(CityDf[City.id,4:5])
points(xNew,xNew,col="grey",pch=11,cex=1.5)
data.frame(xNew,yNew,cost)
}
tol <- 0.0001
maxIter <- 80
df2 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
## Weighted means (tapply does not work for function "weighted.mean)
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xWMean <- sum(w*x)/sum(w)
yWMean <- sum(w*y)/sum(w)
iIter <- 0
xOld <- xWMean
yOld <- yWMean
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
cost <- euclidMedian(CityDf[City.id,4:5])
points(xNew,xNew,col="grey",pch=11,cex=1.5)
data.frame(xNew,yNew,cost)
}
df2
df2 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
## Weighted means (tapply does not work for function "weighted.mean)
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xWMean <- sum(w*x)/sum(w)
yWMean <- sum(w*y)/sum(w)
iIter <- 0
xOld <- xWMean
yOld <- yWMean
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
points(xNew,xNew,col="grey",pch=11,cex=1.5)
data.frame(xNew,yNew)
}
df2
df2 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
## Weighted means (tapply does not work for function "weighted.mean)
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xWMean <- sum(w*x)/sum(w)
yWMean <- sum(w*y)/sum(w)
iIter <- 0
xOld <- xWMean
yOld <- yWMean
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
points(xNew,yNew,col="grey",pch=11,cex=1.5)
data.frame(xNew,yNew)
}
df2
yrs <- 6
t <- seq(0, (yrs*12)-1, by=1)
lambda1 <- 1/12
## Generate time-dependent cos and sin proxy variable
yCos1 <- cos(t*lambda1*2*pi)
ySin1 <- sin(t*lambda1*2*pi)
## Estimate model
yrs.lm <- lm(lungDf$lungDeath~lungDf$monthNum + yCos1 + ySin1 )
load("G:/UTD_Classes/2020Spring/GISC7360_Pattern_Analysis/Lab01/LungDf.RData")
## Estimate model
yrs.lm <- lm(lungDf$lungDeath~lungDf$monthNum + yCos1 + ySin1 )
summary(yrs.lm)
library(MASS)
step.model <- stepAIC(yrs.lm, direction = "both",
trace = FALSE)
summary(step.model)
summary(step.model)
best.lm <- lm(lungDf$lungDeath~ yCos1 + ySin1 )
summary(best.lm)
plot(x = t,y = lungDf$lungDeath,lwd = 2 ,type = "l", col = 'blue')
lines(x = t,y = predict(step.model),lwd = 2,col = 'red')
plot(x = t,y = lungDf$lungDeath,lwd = 2 ,type = "l", col = 'blue')
lines(x = t,y = predict(best.lm),lwd = 2,col = 'red')
xy_ma <- matrix(NA,length(regionSet),2)
regionSet <- unique(CityDf[,"REGIONID"])
xy_ma <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
x.w.mean <- weighted.mean(CityDf[City.id,4],CityDf[City.id,"POPULATION"],na.rm=TRUE)
y.w.mean <- weighted.mean(CityDf[City.id,5],CityDf[City.id,"POPULATION"],na.rm=TRUE)
xy_ma[i,] <-c(x.w.mean,y.w.mean)
}
xy_ma
xy_eu <- matrix(NA,length(regionSet),2)
xy_eu <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
iIter <- 0
xOld <- xy_ma[i,1]
yOld <- xy_ma[i,2]
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
xy_eu[i,] <- c(xNew,yNew)
}
xy_eu
df1 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
## Weighted means (tapply does not work for function "weighted.mean)
x.w.mean <- weighted.mean(CityDf[City.id,4],CityDf[City.id,"POPULATION"],na.rm=TRUE)
y.w.mean <- weighted.mean(CityDf[City.id,5],CityDf[City.id,"POPULATION"],na.rm=TRUE)
points(x.w.mean,y.w.mean,col="red",pch=11,cex=1.5)
data.frame(x.w.mean,y.w.mean)
}
# for (idxReg in regionSet){                                          # cycle over regions
#   City.id <- which(CityDf[,"REGIONID"] == idxReg)                     # get record IDs for particular region
#   ## Weighted means (tapply does not work for function "weighted.mean)
#   x.w.mean <- weighted.mean(CityDf[City.id,4],CityDf[City.id,"POPULATION"],na.rm=TRUE)
#   y.w.mean <- weighted.mean(CityDf[City.id,5],CityDf[City.id,"POPULATION"],na.rm=TRUE)
#   points(x.w.mean,y.w.mean,col="red",pch=11,cex=1.5)                   # Marked weighted mean by cross
# }
library(foreach)
df1 <- foreach::foreach (i = 1:length(regionSet),combind = rbind) %do% {
idxReg <- regionSet[i]
City.id <- which(CityDf[,"REGIONID"] == idxReg)
## Weighted means (tapply does not work for function "weighted.mean)
x.w.mean <- weighted.mean(CityDf[City.id,4],CityDf[City.id,"POPULATION"],na.rm=TRUE)
y.w.mean <- weighted.mean(CityDf[City.id,5],CityDf[City.id,"POPULATION"],na.rm=TRUE)
points(x.w.mean,y.w.mean,col="red",pch=11,cex=1.5)
data.frame(x.w.mean,y.w.mean)
}
df1
tol <- 0.0001
maxIter <- 80
nofPoints <- 10
xy_eu <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
iIter <- 0
xOld <- xy_ma[i,1]
yOld <- xy_ma[i,2]
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
xy_eu[i,] <- c(xNew,yNew)
}
xy_eu
xy_eu <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
iIter <- 0
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
xOld <- xy_ma[i,1]
yOld <- xy_ma[i,2]
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
xy_eu[i,] <- c(xNew,yNew)
}
xy_eu
xy_eu <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
iIter <- 0
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xOld <- xy_ma[i,1]
yOld <- xy_ma[i,2]
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
xy_eu[i,] <- c(xNew,yNew)
}
xy_eu
xy_eu <- matrix(NA,length(regionSet),2)
xy_eu <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
iIter <- 0
City.id <- which(CityDf[,"REGIONID"] == idxReg)
w <- CityDf[City.id,"POPULATION"]
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xOld <- xy_ma[i,1]
yOld <- xy_ma[i,2]
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
print(xyDiff)
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
xy_eu[i,] <- c(xNew,yNew)
}
xy_eu
idxReg
xy_eu <- matrix(NA,length(regionSet),2)
for(i in 1:length(regionSet)){
iIter <- 0
City.id <- which(CityDf[,"REGIONID"] == regionSet[i])
w <- CityDf[City.id,"POPULATION"]
x <- CityDf[City.id,4]
y <- CityDf[City.id,5]
xOld <- xy_ma[i,1]
yOld <- xy_ma[i,2]
repeat{
d <- sqrt((x-xOld)^2+(y-yOld)^2)                       # distance equation in step 1
xNew <- sum(w*x/d)/sum(w/d)                            # update equation in step 2
yNew <- sum(w*y/d)/sum(w/d)
xyDiff <- sqrt((xNew-xOld)^2+(yNew-yOld)^2)            # calculate tolerance score
xOld <- xNew
yOld <- yNew
iIter <- iIter + 1
print(xyDiff)
if (xyDiff < tol | iIter > maxIter) break
Sys.sleep(0.5)
} #end::repeat
xy_eu[i,] <- c(xNew,yNew)
}
xy_eu
install.packages("visdat")
rm(list=ls())
# Helper packages
library(dplyr)
library(ggplot2)
library(visdat)
# Feature engineering packages
library(caret)
library(recipes)
# Loading Dataset
library(AmesHousing); library(rsample)
data("ames_geo")               # gets lat/longs of sales
data("ames_raw")
data("ames_schools")
## Stratified Samples of Sale_Price
set.seed(123)
amesSplit <- initial_split(ames_raw, breaks=8, prop=0.7, strata="SalePrice")
amesTrain <- training(amesSplit)
amesTest <- testing(amesSplit)
# Option 1
## Do the sample log transformation for Sale Price
transformed_response <- log(ames_raw$SalePrice)
par(mfrow = c(1,2))
hist(summary(lm(SalePrice~`Year Built`,data = ames_raw))$residuals,breaks = 50,main = "Non-log transformed model residuals",xlab = 'Value')
hist(summary(lm(transformed_response~ames_raw$`Year Built`))$residuals,breaks = 50,main = "log transformed model residuals",xlab = 'Value')
## Set a recipe for other variables using log transformation
ames_recipe <- recipe(SalePrice~.,data = amesTrain) %>%
step_log(all_outcomes())
ames_recipe
### If we have negative or zero value in original dataset,
### log transformation would produce Inf or NAN, If value <= -1, we could use
### loglp() function to do '+1' offset to all records
cat(log(-0.5),log(0.5),log1p(-0.5))
# Option 2
## Use Box Cox Transformation
summary(car::bcPower())
powerTransform(lm(SalePrice~1, data=ames_raw)))
powerTransform(lm(SalePrice~1, data=ames_raw))
# Option 2
## Use Box Cox Transformation
library(car)
powerTransform(lm(SalePrice~1, data=ames_raw))
lambda <- powerTransform(lm(SalePrice~1, data=ames_raw))$lambda
transformed_response <- bcPower(ames_raw,lambda = lambda)
transformed_response
hist(summary(lm(transformed_response~ames_raw$`Year Built`))$residuals,breaks = 50,main = "log transformed model residuals",xlab = 'Value')
# Dealing with missing data
## Visualizing missing values
sum(is.na(ames_raw))
hist(summary(lm(transformed_response~ames_raw$`Year Built`))$residuals,breaks = 50,main = "box-cox transformed model residuals",xlab = 'Value')
rm(list=ls())
# Helper packages
library(dplyr)
library(ggplot2)
library(visdat)
# Feature engineering packages
library(caret)
library(recipes)
# Loading Dataset
library(AmesHousing); library(rsample)
data("ames_geo")               # gets lat/longs of sales
data("ames_raw")
data("ames_schools")
## Stratified Samples of Sale_Price
set.seed(123)
amesSplit <- initial_split(ames_raw, breaks=8, prop=0.7, strata="SalePrice")
amesTrain <- training(amesSplit)
amesTest <- testing(amesSplit)
# Option 1
## Do the sample log transformation for Sale Price
transformed_response <- log(ames_raw$SalePrice)
par(mfrow = c(1,2))
hist(summary(lm(SalePrice~`Year Built`,data = ames_raw))$residuals,breaks = 50,main = "Non-log transformed model residuals",xlab = 'Value')
hist(summary(lm(transformed_response~ames_raw$`Year Built`))$residuals,breaks = 50,main = "log transformed model residuals",xlab = 'Value')
## Set a recipe for other variables using log transformation
ames_recipe <- recipe(SalePrice~.,data = amesTrain) %>%
step_log(all_outcomes())
ames_recipe
### If we have negative or zero value in original dataset,
### log transformation would produce Inf or NAN, If value <= -1, we could use
### loglp() function to do '+1' offset to all records
cat(log(-0.5),log(0.5),log1p(-0.5))
# Option 2
## Use Box Cox Transformation
library(car)
lambda <- powerTransform(lm(SalePrice~1, data=ames_raw))$lambda
transformed_response <- bcPower(ames_raw,lambda = lambda)
hist(summary(lm(SalePrice~`Year Built`,data = ames_raw))$residuals,breaks = 50,main = "Non-log transformed model residuals",xlab = 'Value')
hist(summary(lm(transformed_response~ames_raw$`Year Built`))$residuals,breaks = 50,main = "box-cox transformed model residuals",xlab = 'Value')
# Dealing with missing data
## Visualizing missing values
sum(is.na(ames_raw))
library(ggplot2)
library(reshape2)
ames_raw %>% is.na() %>%
melt() %>%
ggplot(aes(x = Var2,y = Var1,fill=value)) +
geom_raster() +
coord_flip()+
scale_y_continuous(NULL, expand = c(0,0)) +
scale_fill_grey(name = '',
labels = c("Present",'Missing'))+
xlab('Observation') +
theme(axis.text.y = element_text(size = 4))
## select specific columns not have NA
ames_raw %>%
filter(!is.na('Garage Tyepe')) %>%
select('Garage Type','Garage Cars','Garage Area')
## We could also using vis_miss() function in visdat to vistualize missing data
library(visdat)
vis_miss(ames_raw,cluster = T)
# Imputation
### imputation is the process replacing missing data with a "best guess" data
### Sometime we use mean,median,or other statistics to fill the missing data , but this kind of process
### do not consider any other attributes for a given observation. An alternative is to use grouped statistics
### to capture expected values for observations that fall into similar groups.(infeasible for larger datasets)
## K-nearest neighbor
ames_recipe %>%
step_knnimpute(all_predictors(),neighbors = 6)
caret::nearZeroVar(amesTrain,saveMetrics = T) %>%
rownames_to_column() %>%
filter(nzv)
# Feature filtering
### Pay attention to near-zero variance feature:
### 1. the fraction of unique values over the sample size is low (<10%, eg. 100 obeservation, less than 10 unique values)
### 2. the fraction of the most prevalent value plus the fraction of the second most prevalent value is large. (>20%)
??rownames_to_column
caret::nearZeroVar(amesTrain,saveMetrics = T) %>%
rownames_to_column() %>%
filter(nzv)
library(tibble)
caret::nearZeroVar(amesTrain,saveMetrics = T) %>%
rownames_to_column() %>%
filter(nzv)
amesTrain$Street
unique(amesTrain$Street)
length(unique(amesTrain$Street))/nrow(amesTrain)
length(unique(amesTrain$Street))/nrow(amesTrain) * 100
caret::nearZeroVar(amesTrain,saveMetrics = T) %>%
rownames_to_column() %>%
filter(nzv)
### Equation for Percent Unique
length(unique(amesTrain$Street))/nrow(amesTrain) * 100
?caret::nearZeroVar
table(amesTrain$Street)
sort(table(amesTrain$Street))
sort(table(amesTrain$Street),decreasing = T)
sort(table(amesTrain$Street),decreasing = T)[1]/ sort(table(amesTrain$Street),decreasing = T)[2]
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]/ sort(table(amesTrain$`Land Contour`),decreasing = T)[2] *100%
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]/ (nrow(amesTrain) - sort(table(amesTrain$`Land Contour`),decreasing = T)[1]) *100%
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]
nrow(amesTrain)
nrow(amesTrain) - sort(table(amesTrain$`Land Contour`),decreasing = T)[1]
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]/ (nrow(amesTrain) - sort(table(amesTrain$`Land Contour`),decreasing = T)[1])
table(amesTrain$`Land Contour`)
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]/ sort(table(amesTrain$Street),decreasing = T)[2]
caret::nearZeroVar(amesTrain,saveMetrics = T) %>%
rownames_to_column() %>%
filter(nzv)
sort(table(amesTrain$Street),decreasing = T)[2]
table(amesTrain$Street)
table(amesTrain$Street)
sort(table(amesTrain$`Land Contour`),decreasing = T)[1]/ sort(table(amesTrain$`Land Contour`),decreasing = T)[2]
sort(table(amesTrain$`Land Contour`),decreasing = T)[2]
